# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZgH1sD_mBvxkSaehtAcRmZc8o804kcH_
"""

!pip install gensim

pip install --upgrade numpy pandas gensim

!pip uninstall numpy pandas -y
!pip install --no-cache-dir numpy==1.24.4 pandas==1.5.3

import os
import pickle

# Ensure the 'models' directory exists
os.makedirs('models', exist_ok=True)

# Save model & vectorizer
with open('models/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

import pandas as pd
import numpy as np
import re
import pickle

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# 1. Load dataset
df = pd.read_csv('movie_review.csv')  # Update path if needed
df = df.dropna()

# 2. Basic preprocessing
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text.lower())
    return text

df['clean_review'] = df['text'].astype(str).apply(clean_text)

X = df['clean_review']
y = df['tag']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ========== A. TF-IDF MODEL ==========
print("Training TF-IDF Model...")

tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

model_tfidf = LogisticRegression()
model_tfidf.fit(X_train_tfidf, y_train)

# Evaluate
print("TF-IDF Results:")
print(classification_report(y_test, model_tfidf.predict(X_test_tfidf)))

# Save model & vectorizer
with open('models/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

with open('models/tfidf_model.pkl', 'wb') as f:
    pickle.dump(model_tfidf, f)


# ========== B. WORD2VEC MODEL ==========
print("\nTraining Word2Vec Model...")

# Tokenize for Word2Vec
tokenized_reviews = [simple_preprocess(text) for text in X_train]
w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=2, workers=4)
w2v_model.train(tokenized_reviews, total_examples=len(tokenized_reviews), epochs=10)

# Helper to get sentence vector
def get_avg_vector(text, model, vector_size=100):
    words = simple_preprocess(text)
    valid_words = [word for word in words if word in model.wv]
    if not valid_words:
        return np.zeros(vector_size)
    return np.mean([model.wv[word] for word in valid_words], axis=0)

X_train_w2v = np.array([get_avg_vector(text, w2v_model) for text in X_train])
X_test_w2v = np.array([get_avg_vector(text, w2v_model) for text in X_test])

model_w2v = LogisticRegression(max_iter=1000)
model_w2v.fit(X_train_w2v, y_train)

# Evaluate
print("Word2Vec Results:")
print(classification_report(y_test, model_w2v.predict(X_test_w2v)))

# Save model & embeddings
w2v_model.save("models/word2vec.model")

with open('models/w2v_classifier.pkl', 'wb') as f:
    pickle.dump(model_w2v, f)

print("\nâœ… Both models trained and saved!")

import pandas as pd

df = pd.read_csv("movie_review.csv")  # Adjust path if needed
print(df.columns)
print(df.head())